name: Deploy Full Stack Application to AWS

# Trigger configuration
on:
  workflow_dispatch:  # Manual trigger
  # Uncomment below to enable automated deployments
  # push:
  #   branches: [main]
  #   paths: ["infrastructure/app-infrastructure/**"]
  # pull_request:
  #   branches: [main]
  #   paths: ["infrastructure/app-infrastructure/**"]

# Environment variables
env:
  AWS_REGION: "us-east-1"
  TERRAFORM_VERSION: "1.5.0"
  NODE_VERSION: "21"
  JAVA_VERSION: "17"
  WORKING_DIR_INFRA: "./infrastructure/app-infrastructure"
  WORKING_DIR_DB: "./app-code/database"
  WORKING_DIR_BACKEND: "./app-code/backend"
  WORKING_DIR_FRONTEND: "./app-code/frontend"

# Required permissions
permissions:
  id-token: write
  contents: read

jobs:
  # Infrastructure deployment job
  infrastructure:
    runs-on: ubuntu-latest
    outputs:
      ecr_repository_name: ${{ steps.tf-outputs.outputs.ecr_repository_name }}
      ecs_cluster_name: ${{ steps.tf-outputs.outputs.ecs_cluster_name }}
      ecs_service_name: ${{ steps.tf-outputs.outputs.ecs_service_name }}
      user_pool_id: ${{ steps.tf-outputs.outputs.user_pool_id }}
      user_pool_client_id: ${{ steps.tf-outputs.outputs.user_pool_client_id }}
      frontend_bucket_name: ${{ steps.tf-outputs.outputs.frontend_bucket_name }}
      cloudfront_distribution_id: ${{ steps.tf-outputs.outputs.cloudfront_distribution_id }}
      ecs_security_group_id: ${{ steps.tf-outputs.outputs.ecs_security_group_id }}
      private_subnet_id: ${{ steps.tf-outputs.outputs.private_subnet_id }}
      ecs_task_execution_role_name: ${{ steps.tf-outputs.outputs.ecs_task_execution_role_name }}
      db_address: ${{ steps.tf-outputs.outputs.db_address }}
      app_domain: ${{ steps.tf-outputs.outputs.app_domain }}
      db_migration_log_group_name: ${{ steps.tf-outputs.outputs.db_migration_log_group_name }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v1.7.0
        with:
          role-to-assume: "arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/${{ secrets.AWS_ROLE_TO_ASSUME_NAME }}"
          role-session-name: GitHub_to_AWS_via_FederatedOIDC
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: ${{ env.TERRAFORM_VERSION }}
          terraform_wrapper: false

      - name: Deploy Infrastructure
        working-directory: ${{ env.WORKING_DIR_INFRA }}
        run: |
          terraform init \
            -backend-config="bucket=${{ secrets.TERRAFORM_STATE_BUCKET }}" \
            -backend-config="key=app/terraform.tfstate" \
            -backend-config="region=${{ env.AWS_REGION }}" \
            -backend-config="encrypt=true" \
            -backend-config="dynamodb_table=${{ secrets.TERRAFORM_LOCK_TABLE }}"
          
          terraform plan \
            -var="aws_region=${{ env.AWS_REGION }}" \
            -var="db_name=${{ secrets.DB_NAME }}" \
            -var="db_username=${{ secrets.DB_USERNAME }}" \
            -var="db_password=${{ secrets.DB_PASSWORD }}" \
            -out=tfplan
          
          terraform apply -auto-approve tfplan

      - name: Export Terraform Outputs
        id: tf-outputs
        working-directory: ${{ env.WORKING_DIR_INFRA }}
        run: |
          echo "ecr_repository_name=$(terraform output -raw ecr_repository_name)" >> $GITHUB_OUTPUT
          echo "ecs_cluster_name=$(terraform output -raw ecs_cluster_name)" >> $GITHUB_OUTPUT
          echo "ecs_service_name=$(terraform output -raw ecs_service_name)" >> $GITHUB_OUTPUT
          echo "user_pool_id=$(terraform output -raw user_pool_id)" >> $GITHUB_OUTPUT
          echo "user_pool_client_id=$(terraform output -raw user_pool_client_id)" >> $GITHUB_OUTPUT
          echo "frontend_bucket_name=$(terraform output -raw frontend_bucket_name)" >> $GITHUB_OUTPUT
          echo "cloudfront_distribution_id=$(terraform output -raw cloudfront_distribution_id)" >> $GITHUB_OUTPUT
          echo "ecs_security_group_id=$(terraform output -raw ecs_security_group_id)" >> $GITHUB_OUTPUT
          echo "private_subnet_id=$(terraform output -raw private_subnet_id)" >> $GITHUB_OUTPUT
          echo "ecs_task_execution_role_name=$(terraform output -raw ecs_task_execution_role_name)" >> $GITHUB_OUTPUT
          echo "db_address=$(terraform output -raw db_address)" >> $GITHUB_OUTPUT
          echo "app_domain=$(terraform output -raw app_domain)" >> $GITHUB_OUTPUT
          echo "db_migration_log_group_name=$(terraform output -raw db_migration_log_group_name)" >> $GITHUB_OUTPUT

  # Backend build job - can run in parallel with frontend build
  build-backend:
    runs-on: ubuntu-latest
    needs: infrastructure
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v1.7.0
        with:
          role-to-assume: "arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/${{ secrets.AWS_ROLE_TO_ASSUME_NAME }}"
          role-session-name: GitHub_to_AWS_via_FederatedOIDC
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Java
        uses: actions/setup-java@v3
        with:
          java-version: ${{ env.JAVA_VERSION }}
          distribution: 'temurin'
          cache: 'maven'

      - name: Build Backend
        working-directory: ${{ env.WORKING_DIR_BACKEND }}
        run: |
          mvn wrapper:wrapper
          mvn clean package -DskipTests

      - name: Build and Push Docker Image
        working-directory: ${{ env.WORKING_DIR_BACKEND }}
        run: |
          ECR_REPOSITORY_URL="${{ secrets.AWS_ACCOUNT_ID }}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com/${{ needs.infrastructure.outputs.ecr_repository_name }}"
          aws ecr get-login-password --region ${{ env.AWS_REGION }} | docker login --username AWS --password-stdin $ECR_REPOSITORY_URL
          docker build -t $ECR_REPOSITORY_URL:latest .
          docker push $ECR_REPOSITORY_URL:latest

      - name: Update ECS Service
        run: |
          aws ecs update-service \
            --cluster ${{ needs.infrastructure.outputs.ecs_cluster_name }} \
            --service ${{ needs.infrastructure.outputs.ecs_service_name }} \
            --force-new-deployment

  # Frontend build job - can run in parallel with backend build
  build-frontend:
    runs-on: ubuntu-latest
    needs: infrastructure
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v1.7.0
        with:
          role-to-assume: "arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/${{ secrets.AWS_ROLE_TO_ASSUME_NAME }}"
          role-session-name: GitHub_to_AWS_via_FederatedOIDC
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Node.js
        uses: actions/setup-node@v2
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Build Frontend
        working-directory: ${{ env.WORKING_DIR_FRONTEND }}
        run: |
          cat << EOF > src/environments/environment.prod.ts
          export const environment = {
            production: true,
            cognito: {
              userPoolId: '${{ needs.infrastructure.outputs.user_pool_id }}',
              userPoolClientId: '${{ needs.infrastructure.outputs.user_pool_client_id }}'
            }
          };
          EOF
          
          npm install
          chmod +x ../../scripts/fix-amplify-package.sh
          ../../scripts/fix-amplify-package.sh
          npm run build

      - name: Deploy to S3 and Invalidate Cache
        working-directory: ${{ env.WORKING_DIR_FRONTEND }}
        run: |
          PROJECT_NAME=$(jq -r '.projects | keys | .[0]' angular.json)
          OUTPUT_PATH="$(jq -r ".projects[\"$PROJECT_NAME\"].architect.build.options.outputPath" angular.json)/browser"
          
          aws s3 sync $OUTPUT_PATH/ s3://${{ needs.infrastructure.outputs.frontend_bucket_name }}/
          
          aws cloudfront create-invalidation \
            --distribution-id ${{ needs.infrastructure.outputs.cloudfront_distribution_id }} \
            --paths "/*"

  # Database migration job
  database-migration:
    runs-on: ubuntu-latest
    needs: infrastructure
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v1.7.0
        with:
          role-to-assume: "arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/${{ secrets.AWS_ROLE_TO_ASSUME_NAME }}"
          role-session-name: GitHub_to_AWS_via_FederatedOIDC
          aws-region: ${{ env.AWS_REGION }}

      - name: Generate INSERT-based migration script
        working-directory: ${{ env.WORKING_DIR_DB }}
        run: |
          # Start PostgreSQL service
          sudo service postgresql start
          
          # Create temporary database
          sudo -u postgres createdb temp_migration_db

          # Get absolute path of the csv files
          CSV_PATH="$(pwd)/data/"
          
          # Import database creation script
          sudo -u postgres psql temp_migration_db -v csv_path="$CSV_PATH" < ./init.sql
          
          # Create new dump with INSERT statements
          sudo -u postgres pg_dump --column-inserts temp_migration_db > ./migration.sql
          
          # Clean up
          sudo -u postgres dropdb temp_migration_db
    
      - name: Build and Push Migration Image
        working-directory: ${{ env.WORKING_DIR_DB }}
        run: |
          ECR_REPOSITORY_URL="${{ secrets.AWS_ACCOUNT_ID }}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com/${{ needs.infrastructure.outputs.ecr_repository_name }}"
          MIGRATION_TAG="$ECR_REPOSITORY_URL:migration"
          
          cat > Dockerfile.migration << 'EOF'
          FROM postgres:15
          COPY ./migration.sql /app/database/
          EOF
          
          aws ecr get-login-password --region ${{ env.AWS_REGION }} | docker login --username AWS --password-stdin $ECR_REPOSITORY_URL
          docker build -f Dockerfile.migration -t $MIGRATION_TAG .
          docker push $MIGRATION_TAG

      - name: Run Migration Task
        working-directory: ${{ env.WORKING_DIR_DB }}
        run: |
          ECR_REPOSITORY_URL="${{ secrets.AWS_ACCOUNT_ID }}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com/${{ needs.infrastructure.outputs.ecr_repository_name }}"
          ECS_TASK_EXECUTION_ROLE_ARN="arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/${{ needs.infrastructure.outputs.ecs_task_execution_role_name }}"
          DB_MIGRATION_LOG_GROUP_NAME="${{ needs.infrastructure.outputs.db_migration_log_group_name }}"
          DB_ENDPOINT="${{ needs.infrastructure.outputs.db_address }}"

          TASK_DEF_FAMILY="db-migration-$(date +%s)"
          aws ecs register-task-definition \
            --family $TASK_DEF_FAMILY \
            --network-mode awsvpc \
            --requires-compatibilities FARGATE \
            --cpu 256 \
            --memory 512 \
            --execution-role-arn $ECS_TASK_EXECUTION_ROLE_ARN \
            --container-definitions '[
              {
                "name": "db-migration",
                "image": "'${ECR_REPOSITORY_URL}:migration'",
                "essential": true,
                "environment": [
                  {
                    "name": "PGPASSWORD",
                    "value": "'${{ secrets.DB_PASSWORD }}'"
                  }
                ],
                "logConfiguration": {
                "logDriver": "awslogs",
                "options": {
                  "awslogs-group": "'${DB_MIGRATION_LOG_GROUP_NAME}'",
                  "awslogs-region": "${{ env.AWS_REGION }}",
                  "awslogs-stream-prefix": "ecs"
                  }
                }
              }
            ]'
          
          TASK_ARN=$(aws ecs run-task \
            --cluster ${{ needs.infrastructure.outputs.ecs_cluster_name }} \
            --task-definition $TASK_DEF_FAMILY \
            --launch-type FARGATE \
            --network-configuration "awsvpcConfiguration={subnets=[${{ needs.infrastructure.outputs.private_subnet_id }}],securityGroups=[${{ needs.infrastructure.outputs.ecs_security_group_id }}],assignPublicIp=DISABLED}" \
            --overrides '{
              "containerOverrides": [{
                "name": "db-migration",
                "command": ["psql", "-h", "'$DB_ENDPOINT'", "-p", "5432", "-U", "'${{ secrets.DB_USERNAME }}'", "-d", "'${{ secrets.DB_NAME }}'", "-f", "/app/database/migration.sql"]
              }]
            }' \
            --query 'tasks[0].taskArn' \
            --output text)
          
          echo "Waiting for migration task to complete..."
          aws ecs wait tasks-stopped --cluster ${{ needs.infrastructure.outputs.ecs_cluster_name }} --tasks $TASK_ARN
          
          TASK_STATUS=$(aws ecs describe-tasks \
            --cluster ${{ needs.infrastructure.outputs.ecs_cluster_name }} \
            --tasks $TASK_ARN \
            --query 'tasks[0].containers[0].exitCode' \
            --output text)
          
          aws ecs deregister-task-definition \
            --task-definition $TASK_DEF_FAMILY:1
          
          if [ "$TASK_STATUS" != "0" ]; then
            echo "Migration task failed with status $TASK_STATUS"
            exit 1
          fi

  # Final job
  verify-deployment:
    runs-on: ubuntu-latest
    needs: [build-backend, build-frontend, database-migration]
    steps:
      - name: Check Deployment Status
        run: echo "All components have been deployed successfully"
